# Fluentd Configuration
# Log aggregation and forwarding for ecommerce platform

<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

# Docker logs source
<source>
  @type tail
  path /var/lib/docker/containers/*/*.log
  pos_file /var/log/fluentd/docker.log.pos
  tag docker.*
  read_from_head true
  <parse>
    @type json
    time_key time
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </parse>
</source>

# Application logs - Catalog Service (Python)
<source>
  @type tail
  path /var/log/services/catalog-service/*.log
  pos_file /var/log/fluentd/catalog.log.pos
  tag service.catalog
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </parse>
</source>

# Application logs - Cart Service (Node.js)
<source>
  @type tail
  path /var/log/services/cart-service/*.log
  pos_file /var/log/fluentd/cart.log.pos
  tag service.cart
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </parse>
</source>

# Application logs - Checkout Service (Go)
<source>
  @type tail
  path /var/log/services/checkout-service/*.log
  pos_file /var/log/fluentd/checkout.log.pos
  tag service.checkout
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </parse>
</source>

# Filter: Add service metadata
<filter service.**>
  @type record_transformer
  enable_ruby true
  <record>
    service_name ${tag_parts[1]}
    environment "#{ENV['ENVIRONMENT'] || 'development'}"
    cluster "ecommerce-platform"
    pod_name "#{ENV['HOSTNAME']}"
  </record>
</filter>

# Filter: Parse correlation IDs
<filter service.**>
  @type parser
  key_name message
  reserve_data true
  inject_key_prefix parsed_
  <parse>
    @type regexp
    expression /correlation_id=(?<correlation_id>[^\s]+)/
  </parse>
</filter>

# Filter: Detect log level
<filter service.**>
  @type record_transformer
  enable_ruby true
  <record>
    severity ${record.dig("level") || record.dig("severity") || "INFO"}
  </record>
</filter>

# Filter: Add Kubernetes metadata (when running in k8s)
# <filter service.**>
#   @type kubernetes_metadata
#   @id filter_kube_metadata
#   kubernetes_url "#{ENV['FLUENT_FILTER_KUBERNETES_URL'] || 'https://' + ENV.fetch('KUBERNETES_SERVICE_HOST') + ':' + ENV.fetch('KUBERNETES_SERVICE_PORT') + '/api'}"
#   verify_ssl "#{ENV['KUBERNETES_VERIFY_SSL'] || true}"
#   ca_file "#{ENV['KUBERNETES_CA_FILE']}"
# </filter>

# Filter: Exclude noisy logs
<filter service.**>
  @type grep
  <exclude>
    key message
    pattern /(health|readiness|liveness)/
  </exclude>
</filter>

# Filter: Rate limiting for chatty services
<filter service.catalog>
  @type sampling
  interval 2
  sample_rate 10  # Keep 1 out of every 10 logs if rate > interval
</filter>

# Match: Send to OpenSearch
<match service.**>
  @type opensearch
  host opensearch
  port 9200
  index_name ecommerce-logs
  type_name _doc
  logstash_format true
  logstash_prefix ecommerce
  logstash_dateformat %Y.%m.%d
  include_timestamp true
  reconnect_on_error true
  reload_on_failure true
  reload_connections false
  request_timeout 15s

  <buffer>
    @type file
    path /var/log/fluentd/buffer/opensearch
    flush_mode interval
    flush_interval 5s
    flush_thread_count 2
    retry_type exponential_backoff
    retry_wait 1s
    retry_max_interval 60s
    retry_timeout 60m
    queued_chunks_limit_size 32
    overflow_action block
  </buffer>
</match>

# Match: Send critical errors to separate index
<match service.** tag:error>
  @type copy
  <store>
    @type opensearch
    host opensearch
    port 9200
    index_name ecommerce-errors
    logstash_format true
    logstash_prefix ecommerce-errors
  </store>
  <store>
    @type stdout
  </store>
</match>

# Match: Archive all logs to file
<match **>
  @type file
  path /var/log/fluentd/archive/${tag}/%Y/%m/%d/data
  compress gzip
  <buffer tag,time>
    @type file
    path /var/log/fluentd/buffer/archive
    timekey 86400  # 1 day
    timekey_wait 10m
    flush_mode interval
    flush_interval 30s
  </buffer>
  <format>
    @type json
  </format>
</match>

# System metrics
<source>
  @type monitor_agent
  bind 0.0.0.0
  port 24220
</source>

# Prometheus metrics
<source>
  @type prometheus
  bind 0.0.0.0
  port 24231
  metrics_path /metrics
</source>

<source>
  @type prometheus_output_monitor
  interval 10
  <labels>
    host ${hostname}
  </labels>
</source>
